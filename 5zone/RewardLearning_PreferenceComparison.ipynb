{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  5 11:40:14 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   59C    P8    19W / 100W |     54MiB /  6144MiB |     41%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## COMPROBAR GPU ASIGNADA EN COLABORATORY\n",
    "#########################################################################\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:40:23.813289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## LIBRERIAS NECESARIAS\n",
    "#########################################################################\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import sinergym \n",
    "from sinergym.utils.wrappers import (LoggerWrapper, NormalizeAction,\n",
    "                                     NormalizeObservation) \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "\n",
    "# Librerias necesarias para BC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "# Problema en rollout: es la función que define las transiciones expertas\n",
    "import imitation.data.rollout as rollout \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5Zone**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creamos un vector de entornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==============================================================================================#\n",
      "#==============================================================================================#\n"
     ]
    }
   ],
   "source": [
    "def _make_env():\n",
    "     _env = gym.make(\"Eplus-5zone-hot-discrete-v1\")\n",
    "     _env = NormalizeObservation(_env)\n",
    "     _env = LoggerWrapper(_env)\n",
    "     #_env = RolloutInfoWrapper(_env)\n",
    "     return _env\n",
    "\n",
    "# https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html\n",
    "venv = DummyVecEnv([_make_env for _ in range(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Creación de demostraciones expertas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==============================================================================================#\n",
      "#==============================================================================================#\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Eplus-5zone-hot-discrete-v1\")\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuekve la política experta \n",
    "def download_expert():\n",
    "    print(\"Downloading a pretrained expert.\")\n",
    "    # https://imitation.readthedocs.io/en/latest/main-concepts/experts.html\n",
    "    expert = load_policy(\n",
    "        \"ppo\",\n",
    "        path=\"model5zone.zip\",\n",
    "        venv=env,\n",
    "    )\n",
    "    return expert\n",
    "\n",
    "# Función que devuelve trayectorias de la política experta \n",
    "def sample_expert_transitions():\n",
    "    # Cargamos la política experta\n",
    "    expert = download_expert()\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "\n",
    "    # Generar trayectorias a partir de una política dada\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,  # Política \n",
    "        venv,    # Entorno\n",
    "        sample_until=rollout.make_sample_until(min_timesteps=None, min_episodes=1),  # EPISODES=1 asi que min_episodes=1\n",
    "        rng=np.random.default_rng(),\n",
    "        unwrap=False,\n",
    "    )\n",
    "    \n",
    "    return rollout.flatten_trajectories(rollouts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading a pretrained expert.\n",
      "Sampling expert transitions.\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    }
   ],
   "source": [
    "# Selección de una muestra de trayectorias de secuencias expertas\n",
    "transitions = sample_expert_transitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reward Learning through Preference Comparisons**\n",
    "* El algoritmo de comparación de preferencias aprende una función de recompensa a partir de las preferencias entre pares de trayectorias. Las comparaciones se modelan como generadas a partir de un modelo de Bradley-Terry (o Boltzmann racional), donde la probabilidad de preferir la trayectoria A sobre B es proporcional al exponencial de la diferencia entre el retorno de la trayectoria A menos B. \n",
    "* En otras palabras, la diferencia en los retornos forma un logit para un problema de clasificación binaria, y en consecuencia la función de recompensa se entrena utilizando una pérdida de entropía cruzada para predecir la comparación de preferencias.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_nets import BasicRewardNet \n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "from imitation.util.networks import RunningNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVAL_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, rng=np.random.default_rng())\n",
    "\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=np.random.default_rng())\n",
    "\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=10,\n",
    "    rng=np.random.default_rng(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    ent_coef=0.01,\n",
    "    gae_lambda=0.95,\n",
    "    n_epochs=10,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    }
   ],
   "source": [
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.05,\n",
    "    rng=np.random.default_rng(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=3504, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=7008, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=10512, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=14016, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=17520, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=21024, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=24528, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=28032, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(total_timesteps=31536, total_comparisons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    initial_epoch_multiplier=4,\n",
    "    initial_comparison_frac=0.1,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [20, 51, 41, 34, 29, 25]\n",
      "Collecting 40 fragments (4000 transitions)\n",
      "Requested 3800 transitions but only 0 in buffer. Sampling 3800 additional transitions.\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Sampling 200 exploratory transitions.\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 20 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5031daa92c4428b5db0bceef3784fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2000 timesteps\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| raw/                                 |           |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.88e+03 |\n",
      "|    agent/time/fps                    | 378       |\n",
      "|    agent/time/iterations             | 1         |\n",
      "|    agent/time/time_elapsed           | 5         |\n",
      "|    agent/time/total_timesteps        | 2048      |\n",
      "----------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| mean/                                   |           |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -3.88e+03 |\n",
      "|    agent/time/fps                       | 378       |\n",
      "|    agent/time/iterations                | 1         |\n",
      "|    agent/time/time_elapsed              | 5         |\n",
      "|    agent/time/total_timesteps           | 2.05e+03  |\n",
      "|    agent/train/approx_kl                | 0.00616   |\n",
      "|    agent/train/clip_fraction            | 0.306     |\n",
      "|    agent/train/clip_range               | 0.1       |\n",
      "|    agent/train/entropy_loss             | -2.3      |\n",
      "|    agent/train/explained_variance       | -0.216    |\n",
      "|    agent/train/learning_rate            | 0.002     |\n",
      "|    agent/train/loss                     | -0.0145   |\n",
      "|    agent/train/n_updates                | 10        |\n",
      "|    agent/train/policy_gradient_loss     | -0.0175   |\n",
      "|    agent/train/value_loss               | 0.251     |\n",
      "|    preferences/entropy                  | 0.0336    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.25      |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-0/train/loss            | 2.15      |\n",
      "|    reward/epoch-1/train/accuracy        | 0.3       |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-1/train/loss            | 1.64      |\n",
      "|    reward/epoch-10/train/accuracy       | 0.8       |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-10/train/loss           | 0.316     |\n",
      "|    reward/epoch-11/train/accuracy       | 0.85      |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-11/train/loss           | 0.283     |\n",
      "|    reward/epoch-12/train/accuracy       | 0.85      |\n",
      "|    reward/epoch-12/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-12/train/loss           | 0.253     |\n",
      "|    reward/epoch-13/train/accuracy       | 0.85      |\n",
      "|    reward/epoch-13/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-13/train/loss           | 0.227     |\n",
      "|    reward/epoch-14/train/accuracy       | 0.85      |\n",
      "|    reward/epoch-14/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-14/train/loss           | 0.206     |\n",
      "|    reward/epoch-15/train/accuracy       | 0.9       |\n",
      "|    reward/epoch-15/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-15/train/loss           | 0.187     |\n",
      "|    reward/epoch-16/train/accuracy       | 1         |\n",
      "|    reward/epoch-16/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-16/train/loss           | 0.17      |\n",
      "|    reward/epoch-17/train/accuracy       | 1         |\n",
      "|    reward/epoch-17/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-17/train/loss           | 0.155     |\n",
      "|    reward/epoch-18/train/accuracy       | 1         |\n",
      "|    reward/epoch-18/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-18/train/loss           | 0.141     |\n",
      "|    reward/epoch-19/train/accuracy       | 1         |\n",
      "|    reward/epoch-19/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-19/train/loss           | 0.128     |\n",
      "|    reward/epoch-2/train/accuracy        | 0.35      |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-2/train/loss            | 1.33      |\n",
      "|    reward/epoch-20/train/accuracy       | 1         |\n",
      "|    reward/epoch-20/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-20/train/loss           | 0.117     |\n",
      "|    reward/epoch-21/train/accuracy       | 1         |\n",
      "|    reward/epoch-21/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-21/train/loss           | 0.106     |\n",
      "|    reward/epoch-22/train/accuracy       | 1         |\n",
      "|    reward/epoch-22/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-22/train/loss           | 0.0974    |\n",
      "|    reward/epoch-23/train/accuracy       | 1         |\n",
      "|    reward/epoch-23/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-23/train/loss           | 0.0896    |\n",
      "|    reward/epoch-24/train/accuracy       | 1         |\n",
      "|    reward/epoch-24/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-24/train/loss           | 0.0824    |\n",
      "|    reward/epoch-25/train/accuracy       | 1         |\n",
      "|    reward/epoch-25/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-25/train/loss           | 0.0761    |\n",
      "|    reward/epoch-26/train/accuracy       | 1         |\n",
      "|    reward/epoch-26/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-26/train/loss           | 0.0706    |\n",
      "|    reward/epoch-27/train/accuracy       | 1         |\n",
      "|    reward/epoch-27/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-27/train/loss           | 0.0657    |\n",
      "|    reward/epoch-28/train/accuracy       | 1         |\n",
      "|    reward/epoch-28/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-28/train/loss           | 0.0613    |\n",
      "|    reward/epoch-29/train/accuracy       | 1         |\n",
      "|    reward/epoch-29/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-29/train/loss           | 0.0575    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.45      |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-3/train/loss            | 1.06      |\n",
      "|    reward/epoch-30/train/accuracy       | 1         |\n",
      "|    reward/epoch-30/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-30/train/loss           | 0.0537    |\n",
      "|    reward/epoch-31/train/accuracy       | 1         |\n",
      "|    reward/epoch-31/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-31/train/loss           | 0.0503    |\n",
      "|    reward/epoch-32/train/accuracy       | 1         |\n",
      "|    reward/epoch-32/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-32/train/loss           | 0.0471    |\n",
      "|    reward/epoch-33/train/accuracy       | 1         |\n",
      "|    reward/epoch-33/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-33/train/loss           | 0.0443    |\n",
      "|    reward/epoch-34/train/accuracy       | 1         |\n",
      "|    reward/epoch-34/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-34/train/loss           | 0.0416    |\n",
      "|    reward/epoch-35/train/accuracy       | 1         |\n",
      "|    reward/epoch-35/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-35/train/loss           | 0.039     |\n",
      "|    reward/epoch-36/train/accuracy       | 1         |\n",
      "|    reward/epoch-36/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-36/train/loss           | 0.0367    |\n",
      "|    reward/epoch-37/train/accuracy       | 1         |\n",
      "|    reward/epoch-37/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-37/train/loss           | 0.0345    |\n",
      "|    reward/epoch-38/train/accuracy       | 1         |\n",
      "|    reward/epoch-38/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-38/train/loss           | 0.0325    |\n",
      "|    reward/epoch-39/train/accuracy       | 1         |\n",
      "|    reward/epoch-39/train/gt_reward_loss | 0.0474    |\n",
      "|    reward/epoch-39/train/loss           | 0.0307    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.65      |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-4/train/loss            | 0.835     |\n",
      "|    reward/epoch-5/train/accuracy        | 0.65      |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-5/train/loss            | 0.675     |\n",
      "|    reward/epoch-6/train/accuracy        | 0.75      |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-6/train/loss            | 0.55      |\n",
      "|    reward/epoch-7/train/accuracy        | 0.8       |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-7/train/loss            | 0.464     |\n",
      "|    reward/epoch-8/train/accuracy        | 0.8       |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-8/train/loss            | 0.403     |\n",
      "|    reward/epoch-9/train/accuracy        | 0.8       |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.0474    |\n",
      "|    reward/epoch-9/train/loss            | 0.356     |\n",
      "| reward/                                 |           |\n",
      "|    final/train/accuracy                 | 1         |\n",
      "|    final/train/gt_reward_loss           | 0.0474    |\n",
      "|    final/train/loss                     | 0.0307    |\n",
      "-------------------------------------------------------\n",
      "Collecting 102 fragments (10200 transitions)\n",
      "Requested 9690 transitions but only 0 in buffer. Sampling 9690 additional transitions.\n",
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Sampling 510 exploratory transitions.\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 71 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94c9e6186e142d79484e70a71f8ae93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2000 timesteps\n",
      "Progress: |*--------------------------------------------------------------------------------------------------| 1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.75e+03    |\n",
      "|    agent/time/fps                    | 867          |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0061630034 |\n",
      "|    agent/train/clip_fraction         | 0.306        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.3         |\n",
      "|    agent/train/explained_variance    | -0.216       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0145      |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0175      |\n",
      "|    agent/train/value_loss            | 0.251        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -3.75e+03 |\n",
      "|    agent/time/fps                      | 867       |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 2         |\n",
      "|    agent/time/total_timesteps          | 4.1e+03   |\n",
      "|    agent/train/approx_kl               | 0.00542   |\n",
      "|    agent/train/clip_fraction           | 0.299     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -2.29     |\n",
      "|    agent/train/explained_variance      | 0.46      |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | -0.0422   |\n",
      "|    agent/train/n_updates               | 20        |\n",
      "|    agent/train/policy_gradient_loss    | -0.0166   |\n",
      "|    agent/train/value_loss              | 0.0924    |\n",
      "|    preferences/entropy                 | 0.0407    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.701     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.045     |\n",
      "|    reward/epoch-0/train/loss           | 0.975     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.759     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-1/train/loss           | 0.877     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.749     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0251    |\n",
      "|    reward/epoch-2/train/loss           | 0.494     |\n",
      "|    reward/epoch-3/train/accuracy       | 0.833     |\n",
      "|    reward/epoch-3/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-3/train/loss           | 0.319     |\n",
      "|    reward/epoch-4/train/accuracy       | 0.801     |\n",
      "|    reward/epoch-4/train/gt_reward_loss | 0.0254    |\n",
      "|    reward/epoch-4/train/loss           | 0.305     |\n",
      "|    reward/epoch-5/train/accuracy       | 0.927     |\n",
      "|    reward/epoch-5/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-5/train/loss           | 0.209     |\n",
      "|    reward/epoch-6/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-6/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-6/train/loss           | 0.182     |\n",
      "|    reward/epoch-7/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-7/train/gt_reward_loss | 0.0448    |\n",
      "|    reward/epoch-7/train/loss           | 0.172     |\n",
      "|    reward/epoch-8/train/accuracy       | 0.911     |\n",
      "|    reward/epoch-8/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-8/train/loss           | 0.169     |\n",
      "|    reward/epoch-9/train/accuracy       | 0.958     |\n",
      "|    reward/epoch-9/train/gt_reward_loss | 0.031     |\n",
      "|    reward/epoch-9/train/loss           | 0.115     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.958     |\n",
      "|    final/train/gt_reward_loss          | 0.031     |\n",
      "|    final/train/loss                    | 0.115     |\n",
      "------------------------------------------------------\n",
      "Collecting 82 fragments (8200 transitions)\n",
      "Requested 7790 transitions but only 0 in buffer. Sampling 7790 additional transitions.\n",
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Sampling 410 exploratory transitions.\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 112 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d558f6dd0c4b23bd81071788df773a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2000 timesteps\n",
      "Progress: |*--------------------------------------------------------------------------------------------------| 1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.02e+03    |\n",
      "|    agent/time/fps                    | 880          |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0054165376 |\n",
      "|    agent/train/clip_fraction         | 0.299        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.29        |\n",
      "|    agent/train/explained_variance    | 0.46         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0422      |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0166      |\n",
      "|    agent/train/value_loss            | 0.0924       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.02e+03 |\n",
      "|    agent/time/fps                      | 880       |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 2         |\n",
      "|    agent/time/total_timesteps          | 6.14e+03  |\n",
      "|    agent/train/approx_kl               | 0.00518   |\n",
      "|    agent/train/clip_fraction           | 0.285     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -2.28     |\n",
      "|    agent/train/explained_variance      | 0.643     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | -0.00777  |\n",
      "|    agent/train/n_updates               | 30        |\n",
      "|    agent/train/policy_gradient_loss    | -0.017    |\n",
      "|    agent/train/value_loss              | 0.128     |\n",
      "|    preferences/entropy                 | 0.0497    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.898     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0339    |\n",
      "|    reward/epoch-0/train/loss           | 0.224     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0278    |\n",
      "|    reward/epoch-1/train/loss           | 0.174     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.93      |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0317    |\n",
      "|    reward/epoch-2/train/loss           | 0.157     |\n",
      "|    reward/epoch-3/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-3/train/gt_reward_loss | 0.029     |\n",
      "|    reward/epoch-3/train/loss           | 0.124     |\n",
      "|    reward/epoch-4/train/accuracy       | 0.984     |\n",
      "|    reward/epoch-4/train/gt_reward_loss | 0.0342    |\n",
      "|    reward/epoch-4/train/loss           | 0.115     |\n",
      "|    reward/epoch-5/train/accuracy       | 0.992     |\n",
      "|    reward/epoch-5/train/gt_reward_loss | 0.0259    |\n",
      "|    reward/epoch-5/train/loss           | 0.1       |\n",
      "|    reward/epoch-6/train/accuracy       | 0.984     |\n",
      "|    reward/epoch-6/train/gt_reward_loss | 0.033     |\n",
      "|    reward/epoch-6/train/loss           | 0.0938    |\n",
      "|    reward/epoch-7/train/accuracy       | 0.992     |\n",
      "|    reward/epoch-7/train/gt_reward_loss | 0.026     |\n",
      "|    reward/epoch-7/train/loss           | 0.0769    |\n",
      "|    reward/epoch-8/train/accuracy       | 0.992     |\n",
      "|    reward/epoch-8/train/gt_reward_loss | 0.03      |\n",
      "|    reward/epoch-8/train/loss           | 0.0683    |\n",
      "|    reward/epoch-9/train/accuracy       | 0.984     |\n",
      "|    reward/epoch-9/train/gt_reward_loss | 0.0302    |\n",
      "|    reward/epoch-9/train/loss           | 0.0669    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.984     |\n",
      "|    final/train/gt_reward_loss          | 0.0302    |\n",
      "|    final/train/loss                    | 0.0669    |\n",
      "------------------------------------------------------\n",
      "Collecting 68 fragments (6800 transitions)\n",
      "Requested 6460 transitions but only 0 in buffer. Sampling 6460 additional transitions.\n",
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Sampling 340 exploratory transitions.\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 146 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72473777a5fc4f78936f39d4409431cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2000 timesteps\n",
      "Progress: |*--------------------------------------------------------------------------------------------------| 1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.2e+03     |\n",
      "|    agent/time/fps                    | 832          |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0051789386 |\n",
      "|    agent/train/clip_fraction         | 0.285        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.28        |\n",
      "|    agent/train/explained_variance    | 0.643        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00777     |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.017       |\n",
      "|    agent/train/value_loss            | 0.128        |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.2e+03 |\n",
      "|    agent/time/fps                      | 832      |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 2        |\n",
      "|    agent/time/total_timesteps          | 8.19e+03 |\n",
      "|    agent/train/approx_kl               | 0.00629  |\n",
      "|    agent/train/clip_fraction           | 0.322    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.27    |\n",
      "|    agent/train/explained_variance      | 0.727    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.0277   |\n",
      "|    agent/train/n_updates               | 40       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0196  |\n",
      "|    agent/train/value_loss              | 0.191    |\n",
      "|    preferences/entropy                 | 0.015    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.969    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0233   |\n",
      "|    reward/epoch-0/train/loss           | 0.159    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.956    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0218   |\n",
      "|    reward/epoch-1/train/loss           | 0.128    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.969    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0239   |\n",
      "|    reward/epoch-2/train/loss           | 0.11     |\n",
      "|    reward/epoch-3/train/accuracy       | 0.975    |\n",
      "|    reward/epoch-3/train/gt_reward_loss | 0.0216   |\n",
      "|    reward/epoch-3/train/loss           | 0.0919   |\n",
      "|    reward/epoch-4/train/accuracy       | 0.981    |\n",
      "|    reward/epoch-4/train/gt_reward_loss | 0.0221   |\n",
      "|    reward/epoch-4/train/loss           | 0.0904   |\n",
      "|    reward/epoch-5/train/accuracy       | 0.988    |\n",
      "|    reward/epoch-5/train/gt_reward_loss | 0.0214   |\n",
      "|    reward/epoch-5/train/loss           | 0.0701   |\n",
      "|    reward/epoch-6/train/accuracy       | 0.989    |\n",
      "|    reward/epoch-6/train/gt_reward_loss | 0.0231   |\n",
      "|    reward/epoch-6/train/loss           | 0.0722   |\n",
      "|    reward/epoch-7/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-7/train/gt_reward_loss | 0.0214   |\n",
      "|    reward/epoch-7/train/loss           | 0.0585   |\n",
      "|    reward/epoch-8/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-8/train/gt_reward_loss | 0.0262   |\n",
      "|    reward/epoch-8/train/loss           | 0.0536   |\n",
      "|    reward/epoch-9/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-9/train/gt_reward_loss | 0.0251   |\n",
      "|    reward/epoch-9/train/loss           | 0.0512   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.994    |\n",
      "|    final/train/gt_reward_loss          | 0.0251   |\n",
      "|    final/train/loss                    | 0.0512   |\n",
      "-----------------------------------------------------\n",
      "Collecting 58 fragments (5800 transitions)\n",
      "Requested 5510 transitions but only 0 in buffer. Sampling 5510 additional transitions.\n",
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Sampling 290 exploratory transitions.\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 175 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a7e89f0ce44731bed5cf9e1ba4814d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2000 timesteps\n",
      "Progress: |*--------------------------------------------------------------------------------------------------| 1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.27e+03   |\n",
      "|    agent/time/fps                    | 841         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 10240       |\n",
      "|    agent/train/approx_kl             | 0.006289848 |\n",
      "|    agent/train/clip_fraction         | 0.322       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.27       |\n",
      "|    agent/train/explained_variance    | 0.727       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0277      |\n",
      "|    agent/train/n_updates             | 40          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0196     |\n",
      "|    agent/train/value_loss            | 0.191       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.27e+03 |\n",
      "|    agent/time/fps                      | 841       |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 2         |\n",
      "|    agent/time/total_timesteps          | 1.02e+04  |\n",
      "|    agent/train/approx_kl               | 0.00504   |\n",
      "|    agent/train/clip_fraction           | 0.293     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -2.26     |\n",
      "|    agent/train/explained_variance      | 0.868     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0182    |\n",
      "|    agent/train/n_updates               | 50        |\n",
      "|    agent/train/policy_gradient_loss    | -0.0185   |\n",
      "|    agent/train/value_loss              | 0.219     |\n",
      "|    preferences/entropy                 | 0.0539    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0254    |\n",
      "|    reward/epoch-0/train/loss           | 0.0961    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.974     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0244    |\n",
      "|    reward/epoch-1/train/loss           | 0.0775    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.973     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0245    |\n",
      "|    reward/epoch-2/train/loss           | 0.068     |\n",
      "|    reward/epoch-3/train/accuracy       | 0.995     |\n",
      "|    reward/epoch-3/train/gt_reward_loss | 0.0245    |\n",
      "|    reward/epoch-3/train/loss           | 0.066     |\n",
      "|    reward/epoch-4/train/accuracy       | 0.99      |\n",
      "|    reward/epoch-4/train/gt_reward_loss | 0.0255    |\n",
      "|    reward/epoch-4/train/loss           | 0.0525    |\n",
      "|    reward/epoch-5/train/accuracy       | 0.99      |\n",
      "|    reward/epoch-5/train/gt_reward_loss | 0.0244    |\n",
      "|    reward/epoch-5/train/loss           | 0.0489    |\n",
      "|    reward/epoch-6/train/accuracy       | 0.995     |\n",
      "|    reward/epoch-6/train/gt_reward_loss | 0.0245    |\n",
      "|    reward/epoch-6/train/loss           | 0.0425    |\n",
      "|    reward/epoch-7/train/accuracy       | 0.995     |\n",
      "|    reward/epoch-7/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-7/train/loss           | 0.0394    |\n",
      "|    reward/epoch-8/train/accuracy       | 1         |\n",
      "|    reward/epoch-8/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-8/train/loss           | 0.0363    |\n",
      "|    reward/epoch-9/train/accuracy       | 1         |\n",
      "|    reward/epoch-9/train/gt_reward_loss | 0.0251    |\n",
      "|    reward/epoch-9/train/loss           | 0.0328    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 1         |\n",
      "|    final/train/gt_reward_loss          | 0.0251    |\n",
      "|    final/train/loss                    | 0.0328    |\n",
      "------------------------------------------------------\n",
      "Collecting 50 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Sampling 250 exploratory transitions.\n",
      "Progress: |*******--------------------------------------------------------------------------------------------| 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 200 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297009a97a49451898f3c3c00b11b890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2000 timesteps\n",
      "Progress: |*--------------------------------------------------------------------------------------------------| 1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.34e+03    |\n",
      "|    agent/time/fps                    | 873          |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0050376505 |\n",
      "|    agent/train/clip_fraction         | 0.293        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.26        |\n",
      "|    agent/train/explained_variance    | 0.868        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0182       |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0185      |\n",
      "|    agent/train/value_loss            | 0.219        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.34e+03 |\n",
      "|    agent/time/fps                      | 873       |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 2         |\n",
      "|    agent/time/total_timesteps          | 1.23e+04  |\n",
      "|    agent/train/approx_kl               | 0.00588   |\n",
      "|    agent/train/clip_fraction           | 0.328     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -2.24     |\n",
      "|    agent/train/explained_variance      | 0.867     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0346    |\n",
      "|    agent/train/n_updates               | 60        |\n",
      "|    agent/train/policy_gradient_loss    | -0.0213   |\n",
      "|    agent/train/value_loss              | 0.26      |\n",
      "|    preferences/entropy                 | 0.00994   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.978     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0299    |\n",
      "|    reward/epoch-0/train/loss           | 0.0956    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0213    |\n",
      "|    reward/epoch-1/train/loss           | 0.071     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.982     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0212    |\n",
      "|    reward/epoch-2/train/loss           | 0.0634    |\n",
      "|    reward/epoch-3/train/accuracy       | 0.987     |\n",
      "|    reward/epoch-3/train/gt_reward_loss | 0.0212    |\n",
      "|    reward/epoch-3/train/loss           | 0.0566    |\n",
      "|    reward/epoch-4/train/accuracy       | 0.996     |\n",
      "|    reward/epoch-4/train/gt_reward_loss | 0.0212    |\n",
      "|    reward/epoch-4/train/loss           | 0.0563    |\n",
      "|    reward/epoch-5/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-5/train/gt_reward_loss | 0.0307    |\n",
      "|    reward/epoch-5/train/loss           | 0.0466    |\n",
      "|    reward/epoch-6/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-6/train/gt_reward_loss | 0.0212    |\n",
      "|    reward/epoch-6/train/loss           | 0.0406    |\n",
      "|    reward/epoch-7/train/accuracy       | 0.996     |\n",
      "|    reward/epoch-7/train/gt_reward_loss | 0.0212    |\n",
      "|    reward/epoch-7/train/loss           | 0.0373    |\n",
      "|    reward/epoch-8/train/accuracy       | 0.982     |\n",
      "|    reward/epoch-8/train/gt_reward_loss | 0.022     |\n",
      "|    reward/epoch-8/train/loss           | 0.0589    |\n",
      "|    reward/epoch-9/train/accuracy       | 0.996     |\n",
      "|    reward/epoch-9/train/gt_reward_loss | 0.0265    |\n",
      "|    reward/epoch-9/train/loss           | 0.035     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.996     |\n",
      "|    final/train/gt_reward_loss          | 0.0265    |\n",
      "|    final/train/loss                    | 0.035     |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 0.034957232020263164, 'reward_accuracy': 0.9955357142857142}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"pref_comparisons.train(total_timesteps=35038, total_comparisons=200)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |********-------------------------------------------------------------------------------------------| 8%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |****************************************************************************************************| 100%\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "Reward: -25131 +/- 5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"reward_mean, reward_std = evaluate_policy(agent.policy, venv, N_EVAL_EPISODES)\n",
    "reward_stderr = reward_std/np.sqrt(N_EVAL_EPISODES)\n",
    "\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
