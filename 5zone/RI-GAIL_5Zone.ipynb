{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 23 11:34:56 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   49C    P0    26W / 100W |    150MiB /  6144MiB |     17%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## COMPROBAR GPU ASIGNADA EN COLABORATORY\n",
    "#########################################################################\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.8.3)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.0.106)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.19.3)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3) (12.3.101)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (10.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.49.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 17:41:04.184626: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-22 17:41:04.223389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-22 17:41:04.223412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-22 17:41:04.224116: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-22 17:41:04.228844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 17:41:05.118165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## LIBRERIAS NECESARIAS\n",
    "#########################################################################\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import sinergym \n",
    "from sinergym.utils.wrappers import (LoggerWrapper, NormalizeAction,\n",
    "                                     NormalizeObservation)\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "import copy\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## VARIABLES GLOBALS (HIPERPARÁMETROS)\n",
    "###########################################################################\n",
    "EPOCHS=1\n",
    "\n",
    "EPISODES=10\n",
    "EPISODES_EVALUATE_G=50\n",
    "\n",
    "TOTAL_TIMESTEPS_PPO_GENERATOR=250\n",
    "LEARNING_RATE=0.001\n",
    "ITERATIONS=300  #35039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el entorno\n",
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "# Obtenemos el espacio de estados y acciones del entorno\n",
    "ob_space=env.observation_space\n",
    "ac_space=env.action_space\n",
    "\n",
    "# Mostramos el número de acciones del entorno\n",
    "print(\"\\n Nº de acciones: \", env.action_space.n)\n",
    "# Mostramos el número de observaciones del entorno\n",
    "print(\"\\n Nº deobservaciones: \", ob_space.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminador "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "# Red neuronal del Discriminador\n",
    "################################################################################################################################################\n",
    "\n",
    "# Input: secuencias [s,a,s',r] reales o sintéticas, de longitud 2*ob_space.shape[0] + ac_space.n+1.\n",
    "# Output: probabilidad de que la secuencia sea real, valor perteneciente al intervalo [0,1]\n",
    "discriminator_net=keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(None, 2*ob_space.shape[0] + ac_space.n+1)),\n",
    "        layers.Dense(units=12,activation=tf.nn.relu, name='layer1'),\n",
    "        layers.Dense(units=12,activation=tf.nn.relu, name='layer2'),\n",
    "        layers.Dense(units=12, activation=tf.nn.relu, name='layer3'),\n",
    "        layers.Dense(units=1, activation=tf.sigmoid, name='prob'),\n",
    "\n",
    "    ],\n",
    "    name=\"discriminator_net\"\n",
    "\n",
    ")\n",
    "discriminator_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de pérdida del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# Función de pérdida del Discriminador\n",
    "#########################################################################################################\n",
    "\n",
    "# prob1=> output de la red neuronal del Discriminador cuando recibe como entrada una secuencia REAL [s,a] de la base de datos\n",
    "# prob2=> output de la red neuronal del Discriminador cuando recibe como entrada una secuencia FALSA [s,a]\n",
    "def loss_fn_D(prob1, prob2):\n",
    "\n",
    "    # Esperanza del logaritmo de la D(x)=salida de la red neuronal cuando x=entrada REAL\n",
    "    loss_expert = tf.reduce_mean(tf.math.log(tf.clip_by_value(prob1, 0.01, 1)))\n",
    "\n",
    "    # Esperanza del logaritmo de 1-D(x) donde D(x)=salida de la red neuronal cuando x=entrada FALSA\n",
    "    loss_agent = tf.reduce_mean(tf.math.log(tf.clip_by_value(1 - prob2, 0.01,1)))\n",
    "\n",
    "    loss_expert = tf.cast(loss_expert, dtype=tf.float32)\n",
    "    loss_agent = tf.cast(loss_agent, dtype=tf.float32)\n",
    "\n",
    "    loss = loss_expert + loss_agent\n",
    "\n",
    "    loss = -loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase del Discriminador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "#  DISCRIMINADOR \n",
    "################################################################################################\n",
    "class Discriminator:\n",
    "    def __init__(self, env, discriminator_net, expert_s, expert_a, expert_s_prima, expert_r, agent_s, agent_a, agent_s_prima, agent_r):\n",
    "        # -Red neuronal del Discriminador\n",
    "        self.discriminator_net=discriminator_net\n",
    "\n",
    "        # - 1) Secuencia experto: [s,a,s',r]\n",
    "        self.expert_s=expert_s\n",
    "        self.expert_a=expert_a\n",
    "        self.expert_s_prima=expert_s_prima\n",
    "        self.expert_r= np.array(expert_r)\n",
    "\n",
    "\n",
    "        expert_a_one_hot=tf.one_hot(self.expert_a,depth=env.action_space.n)\n",
    "        # Añadimos ruido para estabilizar el entrenamiento\n",
    "        expert_a_one_hot+= tf.random.normal(tf.shape(expert_a_one_hot), mean=0.2, stddev=0.1, dtype=tf.float32)/1.2\n",
    "        expert_s_a=tf.concat([self.expert_s,expert_a_one_hot],axis=1)\n",
    "        expert_s_a_s=tf.concat([expert_s_a, self.expert_s_prima], axis=1)\n",
    "        expert_r=self.expert_r.reshape(-1, 1)\n",
    "\n",
    "        # expert_s_a_s_r=>secuencia experta=>[s,a,s',r]\n",
    "        self.expert_s_a_s_r=tf.concat([expert_s_a_s, expert_r], axis=1)\n",
    "\n",
    "        # -2) Secuencia agente:  [s,a,s',r]\n",
    "        self.agent_s=agent_s\n",
    "        self.agent_a=agent_a\n",
    "        self.agent_s_prima=agent_s_prima\n",
    "        self.agent_r=np.array(agent_r)\n",
    "\n",
    "        agent_a_one_hot=tf.one_hot(self.agent_a,depth=env.action_space.n)\n",
    "        agent_a_one_hot+= tf.random.normal(tf.shape(agent_a_one_hot), mean=0.2, stddev=0.1, dtype=tf.float32)/1.2\n",
    "        agent_s_a=tf.concat([self.agent_s,agent_a_one_hot],axis=1)\n",
    "        agent_s_a_s=tf.concat([agent_s_a, self.agent_s_prima], axis=1)\n",
    "        agent_r=self.agent_r.reshape(-1, 1)\n",
    "\n",
    "        # agent_s_a_s_r=>secuencia agente=>[s,a,s',r]\n",
    "        self.agent_s_a_s_r=tf.concat([agent_s_a_s, agent_r], axis=1)\n",
    "\n",
    "        # Calculamos la salida de la red para [s,a,s',r] del experto y del agente ya que lo necesitamos para reward\n",
    "\n",
    "        # -Salida de la red neuronal Discriminador para [s,a,s',r] expertos(verdaderos)\n",
    "        self.prob_expert=self.discriminator_net(self.expert_s_a_s_r)\n",
    "\n",
    "        # -Salida  de la red neuronal Discrimiinador para [s,a,s',r] Agente(falsos)\n",
    "        self.prob_agent=self.discriminator_net(self.agent_s_a_s_r)\n",
    "\n",
    "        # -Recompensa obtenida cuando el Agente realiza [s,a,s',r] falsas\n",
    "        self.rewards=tf.math.log(tf.clip_by_value(self.prob_agent,1e-10,1)) #log(P(expert|s,a)) cuando mas grande es mejor el agente\n",
    "\n",
    "\n",
    "    def getNet(self):\n",
    "        return self.discriminator_net\n",
    "\n",
    "    def getAgent_S_A(self):\n",
    "        return self.agent_s_a\n",
    "\n",
    "    def getExpert_S_A(self):\n",
    "        return self.expert_s_a\n",
    "\n",
    "    def getProb(self):\n",
    "        return self.prob_expert, self.prob_agent\n",
    "\n",
    "    def getRewards(self):\n",
    "        return self.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales del Generador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_net_Act\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, None, 17)          306       \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, None, 17)          306       \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, None, 13)          234       \n",
      "                                                                 \n",
      " layer4 (Dense)              (None, None, 10)          140       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986 (3.85 KB)\n",
      "Trainable params: 986 (3.85 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "# Red neuronal del Generador donde se producen acciones\n",
    "####################################################################################################\n",
    "\n",
    "# Input: estados, listas de tamaño 17, s=[s1,s2,s3,....,s17]\n",
    "# Output: acciones, listas de tamaño 10, a=[a1,a2,....,a10]\n",
    "generator_net_Act = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(None, ob_space.shape[0])),\n",
    "        layers.Dense(units=15, activation=tf.tanh, name='layer1'),\n",
    "        layers.Dense(units=15, activation=tf.tanh, name='layer2'),\n",
    "        layers.Dense(units=13, activation=tf.tanh, name='layer3'),\n",
    "        layers.Dense(units=ac_space.n, activation=tf.nn.softmax, name='layer4')\n",
    "\n",
    "    ],\n",
    "    name=\"generator_net_Act\"\n",
    ")\n",
    "\n",
    "generator_net_Act.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_v_preds\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, None, 14)          252       \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, None, 14)          210       \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, None, 1)           15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 477 (1.86 KB)\n",
      "Trainable params: 477 (1.86 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# Red neuronal del Generador donde se producen v_pred\n",
    "###################################################################################\n",
    "# Input: estados, listas de tamaño 17, s=[s1,s2,s3,....,s17]\n",
    "# Output: v_pred, listas de tamaño 1, v_pred(s)\n",
    "\n",
    "generator_net_v_preds = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(None, ob_space.shape[0])),\n",
    "        layers.Dense(units=14, activation=tf.tanh, name='layer1'),\n",
    "        layers.Dense(units=14, activation=tf.tanh, name='layer2'),\n",
    "        layers.Dense(units=1, activation=None, name='layer3'),\n",
    "    ],\n",
    "    name=\"generator_v_preds\"\n",
    ")\n",
    "\n",
    "generator_net_v_preds.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de pérdida del Generador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# Función de pérdida del Generador: función objetivo de PPO \"clipped surrogated\" \n",
    "#################################################################################################################\n",
    "def loss_fn_ppo(act_probs,act_probs_old,gaes,clip_value=0.2):\n",
    "    ratios = tf.exp(tf.math.log(tf.clip_by_value(act_probs, 1e-10, 1.0))\n",
    "                    - tf.math.log(tf.clip_by_value(act_probs_old, 1e-10, 1.0)))\n",
    "\n",
    "    clipped_ratios = tf.clip_by_value(ratios,clip_value_min=1 -clip_value,clip_value_max=1 +clip_value)\n",
    "    loss_clip = tf.minimum( tf.multiply(gaes, ratios), tf.multiply(gaes, clipped_ratios))\n",
    "    loss_clip = tf.reduce_mean(loss_clip)\n",
    "\n",
    "    loss = -loss_clip\n",
    "    tf.summary.scalar('total', loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase del Generador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# Clase del GENERADOR: política con su optimizador PPO\n",
    "################################################################################################################\n",
    "\n",
    "# Observesé que cada generador implementa una política distinta, por tanto, se ha decidido llamar a la clase Policy_net en lugar de generator\n",
    "class Policy_net:\n",
    "    def __init__(self, name: str, env, obs):\n",
    "        \"\"\"\n",
    "        name: string\n",
    "        env: gym env\n",
    "        obs:\n",
    "        \"\"\"\n",
    "\n",
    "        # -Entorno\n",
    "        self.env=env\n",
    "        env.reset()\n",
    "\n",
    "        # -Modelo PPO: algoritmo de Optimización de Política Proximal\n",
    "        self.model=PPO(policy=\"MlpPolicy\", env=env, verbose=0)\n",
    "\n",
    "\n",
    "        self.model.learn(total_timesteps=TOTAL_TIMESTEPS_PPO_GENERATOR)\n",
    "\n",
    "        # -Observación inicial a partir de la cual se crean las acciones iniciales haciendo uso de las redes neuronales del generador\n",
    "        self.obs=np.reshape(np.array(obs),(1,ob_space.shape[0]))\n",
    "\n",
    "        # Utilizamos las dos redes neuronales que hemos creado : generator_net_Act y generator_net_v_preds\n",
    "        # V_pred=>recompensa media de que un agente ejecute una acción\n",
    "\n",
    "        # -Acción inicial generada con red neuronal y v_pred con red neuronal\n",
    "        self.act_probs =generator_net_Act(self.obs)\n",
    "        self.v_preds = generator_net_v_preds(self.obs)\n",
    "\n",
    "        # -Accion estocástica inicial\n",
    "        self.act_stochastic = tf.random.categorical(tf.math.log(self.act_probs), num_samples=1)\n",
    "\n",
    "        # -Acción determinística inicial\n",
    "        self.act_deterministic = tf.argmax(self.act_probs, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Para cada estado obs me dice la acción que el agente va a ejecutar sobre el entorno junto con v_pred\n",
    "    # La elección de la acción puede ser estocástica o determinística\n",
    "    def act(self, stochastic=True):\n",
    "        if stochastic:\n",
    "            return self.act_stochastic, self.v_preds\n",
    "        else:\n",
    "            return self.act_deterministic, self.v_preds\n",
    "\n",
    "    def get_action_prob(self):\n",
    "        return self.act_probs\n",
    "\n",
    "    def get_v_preds(self):\n",
    "        return self.v_preds\n",
    "\n",
    "    def get_obs(self):\n",
    "        return self.obs\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    # Devuelve los parámetros \\theta de la política \\pi\n",
    "    def get_trainable_variables(self):\n",
    "        return self.model.get_parameters()\n",
    "\n",
    "    # Generar [s,a,s',r] sinteticos\n",
    "    def generate_fakes(self):\n",
    "\n",
    "        ob_space = env.observation_space\n",
    "        reward = 0\n",
    "        success_num = 0\n",
    "\n",
    "\n",
    "        # Por cada episodio\n",
    "        for iteration in range(EPISODES):\n",
    "            # Inicializo todas las variables\n",
    "            observations = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            next_observations=[]\n",
    "\n",
    "            run_policy_steps = 0\n",
    "\n",
    "            truncated=False\n",
    "            terminated=False\n",
    "\n",
    "\n",
    "            # La primera acción de cada episodio se crea con la red neuronal\n",
    "            obs,_=env.reset()\n",
    "\n",
    "            Old_Policy = Policy_net('old_policy', env, obs=obs)\n",
    "\n",
    "            act, v_pred = Old_Policy.act(stochastic=True)\n",
    "\n",
    "            #Convertir de tensor a array\n",
    "            if type(act)=='Tensor':\n",
    "                # Crear una sesión de TensorFlow\n",
    "                sess = tf.compat.v1.Session()\n",
    "\n",
    "                # Evaluar el tensor dentro de la sesión y obtener el resultado como un objeto NumPy ndarray\n",
    "                act = sess.run(act)\n",
    "\n",
    "                # Cerrar la sesión\n",
    "                sess.close()\n",
    "\n",
    "            if isinstance(act, tf.Tensor):\n",
    "                act=act.numpy()\n",
    "\n",
    "            elif isinstance(act, np.ndarray):\n",
    "                act=act\n",
    "\n",
    "\n",
    "            action=int(act)\n",
    "\n",
    "            next_obs,reward,terminated,truncated, info=env.step(action)\n",
    "\n",
    "            truncated=False\n",
    "            terminated=False\n",
    "\n",
    "            # Tenemos una política entrenada\n",
    "            Policy = Policy_net('policy',env, obs=[next_obs])\n",
    "\n",
    "            # Por cada steps en cada episodio, mientras no se llegue a un estado terminal o un estado malo\n",
    "            while terminated!= True and truncated!= True:\n",
    "                # --Aumentar el numero de steps\n",
    "                run_policy_steps += 1\n",
    "\n",
    "                # --Política para ver la acción asociada al estado\n",
    "                # Las observaciones son un de la forma [[s_1,s_2,s_3,...,s_8]] por eso su tamaño es (1,8)\n",
    "                observations.append(next_obs)  # S_i-1\n",
    "\n",
    "                action, states_oc = Policy.get_model().predict(next_obs)\n",
    "\n",
    "                action=int(action)\n",
    "\n",
    "                # --Muevo al Agente al siguiente estado\n",
    "                next_obs,reward,terminated,truncated,info=env.step(action)\n",
    "\n",
    "                # --Actualización de variables\n",
    "                actions.append(action) # A_i-1\n",
    "                rewards.append(reward) # R_i-1\n",
    "\n",
    "                # --Si llegamos a un estado final, el juego ha finalizado!!!\n",
    "                # --Se configura el tablero de nuevo\n",
    "                if terminated== True  or truncated==True:\n",
    "                    next_observations.append(next_obs)  # S_i\n",
    "                    obs = env.reset()\n",
    "                    reward = -1\n",
    "                    break\n",
    "                else:\n",
    "                    next_observations.append(next_obs)  # S_i\n",
    "                    self.obs = next_obs\n",
    "\n",
    "            # Ver si el episodio ha obtendo una recompensa total igual o superior a 195\n",
    "            if sum(rewards) >= 195:\n",
    "                success_num += 1\n",
    "                if success_num >= 100:\n",
    "                    break\n",
    "            else:\n",
    "                success_num = 0\n",
    "\n",
    "\n",
    "        observations = np.reshape(observations, newshape=[-1] + list(ob_space.shape))\n",
    "        next_observations = np.reshape(next_observations, newshape=[-1] + list(ob_space.shape))\n",
    "        actions = np.array(actions).astype(dtype=np.int32)\n",
    "\n",
    "\n",
    "        # Devolvemos la secuencia (S,A,S',R) junto con la política anterior y la actual política,\n",
    "        return observations, actions, next_observations, rewards, Old_Policy, Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "# Clase PPOTrain\n",
    "##########################################################################################################\n",
    "# Tenemos dos politica theta_i y theta_i+1\n",
    "# Almacenamos dos políticas Policy_net(cada una de ella con su PPO) y calculamos el valor gaes a partir de valores gamma, clip_value, c_1, c_2\n",
    "# Realizamos aqui el entrenamiento, cálculo de gradiente y función de pérdida del PPO para después usarlo en el generador de la GAN\n",
    "\n",
    "class PPOTrain:\n",
    "\n",
    "    def __init__(self, Policy, Old_Policy, obs, actions, rewards, gamma=0.95, clip_value=0.2, c_1=1, c_2=0.01):\n",
    "        \"\"\"\n",
    "        arg:\n",
    "            Policy\n",
    "            Old_Policy\n",
    "            gamma\n",
    "            clip_value\n",
    "            c_1 parámetro para la diferencia de valores\n",
    "            c_2 parámetro para el bonus de entropía\n",
    "        \"\"\"\n",
    "        self.Policy = Policy\n",
    "        self.Old_Policy = Old_Policy\n",
    "        self.gamma = gamma\n",
    "        self.obs=obs\n",
    "\n",
    "        self.pi_trainable = self.Policy.get_trainable_variables()\n",
    "        self.old_pi_trainable = self.Old_Policy.get_trainable_variables()\n",
    "\n",
    "\n",
    "        policy_name = \"policy\"\n",
    "        old_policy_name=\"policy\"\n",
    "\n",
    "        policy_dict_ = self.pi_trainable[policy_name]\n",
    "        old_policy_dict_=self.old_pi_trainable[old_policy_name]\n",
    "\n",
    "        self.pi=[]\n",
    "        if policy_name in self.pi_trainable and old_policy_name in self.old_pi_trainable:\n",
    "            for param_name, param_value in policy_dict_.items():\n",
    "                # Elimino los pesos que hay en old_policy\n",
    "                del old_policy_dict_[param_name]\n",
    "                # Introduzco los pesos de old_policy en policy\n",
    "                old_policy_dict_[param_name] = param_value\n",
    "                self.pi.append(param_value)\n",
    "        else:\n",
    "            print(f\"No se encontró la política con el nombre: {policy_name}\")\n",
    "\n",
    "\n",
    "        # Le asignamos old_pi_trainable=pi_trainable ya que ajustaremos unos nuevos pi_trainable\n",
    "\n",
    "\n",
    "        self.actions = actions\n",
    "        self.rewards=rewards\n",
    "        self.v_preds=self.Old_Policy.get_v_preds()\n",
    "        self.v_preds_next=self.Policy.get_v_preds()\n",
    "\n",
    "        #  generative advantage estimator(lambda = 1), ver ppo paper eq(11)\n",
    "        self.gaes =self.get_gaes(self.rewards, self.v_preds, self.v_preds_next)\n",
    "\n",
    "        act_probs =self.Policy.get_action_prob()\n",
    "        act_probs_old =self.Old_Policy.get_action_prob()\n",
    "\n",
    "        # la probabilidad de las acciones del agente cuando toma la actual política\n",
    "        act_probs = act_probs * tf.one_hot(indices=self.actions, depth=act_probs.shape[1])\n",
    "        self.act_probs = tf.reduce_sum(act_probs, axis=1)\n",
    "\n",
    "        # la probabilidad de las acciones del agente cuando toma la antigua política\n",
    "        act_probs_old = act_probs_old * tf.one_hot(indices=self.actions, depth=act_probs_old.shape[1])\n",
    "        self.act_probs_old = tf.reduce_sum(act_probs_old, axis=1)\n",
    "\n",
    "        self.loss=loss_fn_ppo(self.act_probs, self.act_probs_old, self.gaes)\n",
    "\n",
    "        self.optimizer =tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    def loss_fn_G(self):\n",
    "        return loss_fn_ppo(self.act_probs, self.act_probs_old, self.gaes)\n",
    "\n",
    "    def get_pi_trainable(self):\n",
    "        return self.pi\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_OldPolicy(self):\n",
    "        return self.Old_Policy\n",
    "\n",
    "    def get_Policy(self):\n",
    "        return self.Policy\n",
    "\n",
    "    def get_gaes(self, rewards, v_preds, v_preds_next):\n",
    "        deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]\n",
    "        # calcular la estimación generative advantage (lambda = 1), ver ppo paper eq(11)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(gaes) - 1)):  # es T-1, donde T es timestep con el que se ejecuta la política\n",
    "            gaes[t] = gaes[t] + self.gamma * gaes[t + 1]\n",
    "        return gaes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RI GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# CLASE RI-GAIL\n",
    "####################################################################################################################\n",
    "class GAN(keras.Model):\n",
    "    # Constructor\n",
    "    def __init__(self, discriminator, generator):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator=generator\n",
    "        self.i=0\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    # Compila el modelo GAN inicializando los optimizadores y la función de pérdida del modelo GAN\n",
    "    def compile(self,d_optimizer, loss_fn_D ):\n",
    "        super(GAN, self).compile(run_eagerly=True)\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_fn_D=  loss_fn_D\n",
    "\n",
    "    # Devuelve las métricas obtenidas con el generador y discriminador\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric,self.g_loss_metric]\n",
    "\n",
    "    # Evaluación del Discriminador\n",
    "    def evaluate_D(self, X_test):\n",
    "        len_real = X_test.shape[0]\n",
    "\n",
    "        generate_observations, generate_actions, generate_next_observations, rewards, Old_Policy, Policy=self.generator.generate_fakes()\n",
    "\n",
    "        generate_a_one_hot=np.eye(env.action_space.n)[generate_actions]\n",
    "\n",
    "        dataset_gen1=np.concatenate([generate_observations,generate_a_one_hot],axis=1)\n",
    "\n",
    "        dataset_gen2= np.concatenate([dataset_gen1,generate_next_observations], axis=1)\n",
    "\n",
    "        rewards=np.array(rewards).reshape(-1, 1)\n",
    "\n",
    "        dataset_gen= np.concatenate([dataset_gen2,rewards], axis=1)\n",
    "\n",
    "\n",
    "        len_fakes=dataset_gen.shape[0]\n",
    "\n",
    "        # Compilamos el discriminador como CNN\n",
    "        self.discriminator.discriminator_net.compile(optimizer=self.d_optimizer, loss=self.loss_fn_D, metrics=['accuracy'])\n",
    "\n",
    "        # Evaluamos como CNN\n",
    "        loss_real, acc_real=self.discriminator.discriminator_net.evaluate(X_test, tf.ones((len_real,1)), batch_size=len_real, verbose=1)\n",
    "\n",
    "        loss_fake, acc_fake=self.discriminator.discriminator_net.evaluate(dataset_gen,tf.ones((len_fakes,1)), batch_size=len_fakes, verbose=1)\n",
    "\n",
    "        print('>Loss real: ')\n",
    "        print(loss_real)\n",
    "        print('>Loss fake: ')\n",
    "        print(loss_fake)\n",
    "\n",
    "\n",
    "    # Evaluación del generador\n",
    "    def evaluate_G(self):\n",
    "        # Definimos el entorno\n",
    "        env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "        env = NormalizeObservation(env)\n",
    "        env = LoggerWrapper(env)\n",
    "\n",
    "        # Lista donde amacenaremos la recompensa acumulada de cada episodio.\n",
    "        # NUESTRO OBJETIVO: Agente aprenda a tomar las acciones que maximicen la recompensa\n",
    "        rewards=[]\n",
    "\n",
    "        # Para cada episodio, el Agente se mueve por el Entorno mediante acciones hasta llegar a un estado final\n",
    "        # siguiendo la política que se ha aprendido en el entrenamiento de la GAN\n",
    "        for episode in range(EPISODES_EVALUATE_G):\n",
    "            truncated=False\n",
    "            terminated=False\n",
    "            R=0.0\n",
    "            reward=0.0\n",
    "\n",
    "            # Estado inicial del juego\n",
    "            obs,_=env.reset()\n",
    "\n",
    "            #Interactuamos con el Entorno hasta que lleguemos a un estado final\n",
    "            while terminated!= True and truncated!=True:\n",
    "                action, _=self.generator.get_model().predict(obs)\n",
    "                obs,reward,terminated,truncated, info=env.step(int(action))\n",
    "\n",
    "                # Incremento la recompensa del episodio i al haber ejecutado el step\n",
    "                R+=reward\n",
    "\n",
    "            rewards.append(R)\n",
    "\n",
    "            # Vemos para el episodio, su recompensa acumulada que es lo que se trata de maximizar\n",
    "            print(\"Episode  {} Total reward: {}\".format(episode,R))\n",
    "\n",
    "        # Cierro el entorno\n",
    "        env.close()\n",
    "\n",
    "        # Muestro las recompensas obtenidas en cada episodio\n",
    "        indices = range(0, EPISODES_EVALUATE_G)\n",
    "        plt.plot(indices,rewards)\n",
    "        plt.show()\n",
    "\n",
    "        return np.mean(rewards)\n",
    "\n",
    "    def train_step(self, X_train):\n",
    "\n",
    "        # 1) Generamos secuencias falsas [s,a,s',r]\n",
    "        generate_observations, generate_actions, generate_next_observations, rewards, Old_Policy, Policy=self.generator.generate_fakes()\n",
    "\n",
    "        generate_a_one_hot=np.eye(env.action_space.n)[generate_actions]\n",
    "\n",
    "        if generate_observations.shape[0] == generate_a_one_hot.shape[0]:\n",
    "          dataset_gen1 = np.concatenate([generate_observations, generate_a_one_hot], axis=1)\n",
    "        else:\n",
    "          generate_a_one_hot_resized = np.resize(generate_a_one_hot, generate_observations.shape)\n",
    "          dataset_gen1 = np.concatenate([generate_observations, generate_a_one_hot_resized], axis=1)\n",
    "\n",
    "        if generate_next_observations.shape[0] == generate_a_one_hot.shape[0]:\n",
    "          dataset_gen2=np.concatenate([dataset_gen1, generate_next_observations], axis=1)\n",
    "        else:\n",
    "           generate_next_observations_new= np.resize(generate_next_observations, generate_observations.shape)\n",
    "           dataset_gen2=np.concatenate([dataset_gen1, generate_next_observations_new], axis=1)\n",
    "\n",
    "        rewards=np.array(rewards).reshape(-1,1)\n",
    "\n",
    "        dataset_gen=np.concatenate([dataset_gen2, rewards], axis=1)\n",
    "\n",
    "        # 2) Seleccionamos la muestra de datos generador con la que vamos a trabajar en este  batch de entrenamiento \n",
    "        #if len(dataset_gen) >= BATCH_SIZE: \n",
    "        random_indices = np.random.choice(len(dataset_gen), size=min(BATCH_SIZE,len(dataset_gen)), replace=False)\n",
    "        dataset_gen= dataset_gen[random_indices[0]]\n",
    "\n",
    "\n",
    "        # 3) Obtenemos las secuencias reales [s,a,s',r] de los datos de entrenamiento y las combinamos\n",
    "        dataset_gen=dataset_gen.reshape(1,-1)\n",
    "        combined_images = tf.concat([X_train, dataset_gen], axis=0)\n",
    "\n",
    "        # 4) Las etiquetas de las imagenes combinadas las tenemos que crear nosotros introduciendo algo de ruido con tf.random.uniform\n",
    "        labels = tf.concat( [tf.ones((BATCH_SIZE, 1)), tf.zeros((BATCH_SIZE, 1))], axis=0 )\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        ##############################################################################################################################################################\n",
    "        # PASO 1:  ENTRENAMIENTO DEL DISCRIMINADOR\n",
    "        #############################################################################################################################################################\n",
    "\n",
    "\n",
    "        # Entrenamiento del discriminador con las [s,a, s', r] del agente(falsas o sintéticas) y del experto (reales) combinadas, esto es,\n",
    "        # le pasamos un conjunto que tiene tanto secuencias reales como secuencias sintéticas\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions=np.zeros((2*BATCH_SIZE,6))\n",
    "            # Predicciones obtenidas con el discriminador\n",
    "            predictions = self.discriminator.discriminator_net(combined_images)\n",
    "            # Valor de la función de pérdida al comparar las predicciones con las etiquetas reales\n",
    "            d_loss = self.loss_fn_D(labels, predictions)\n",
    "\n",
    "        # Calculo del gradiente y actualización del gradiente\n",
    "        grads = tape.gradient(d_loss, self.discriminator.getNet().trainable_weights)\n",
    "\n",
    "        self.d_optimizer.apply_gradients(\n",
    "          zip(grads, self.discriminator.getNet().trainable_weights)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # PASO 2: ENTRENAMIENTO DEL GENERADOR=POLÍTICA\n",
    "        ###############################################################################################################################################################\n",
    "\n",
    "\n",
    "        ppotrain=PPOTrain(Policy,Old_Policy,actions=generate_actions,rewards=rewards, obs=generate_observations) #       generate_observations[0])\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_loss = ppotrain.loss_fn_G()\n",
    "\n",
    "\n",
    "        g_loss = tf.cast(g_loss, dtype=tf.float32)\n",
    "\n",
    "         ############################################################################################################################################################\n",
    "\n",
    "        # Actualización de métricas del discriminador y generador\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "\n",
    "        return {\"d_loss\": self.d_loss_metric.result(),\n",
    "                    \"g_loss\": self.g_loss_metric.result()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentación de RIGAIL con 5Zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "# Deshabilitar las advertencias de tipo UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Deshabilitar las advertencias de tipo FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria = np.count_nonzero(expert_num_tray == 0)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan1=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan1.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan1.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan1.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan1.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 1)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan2=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan2.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan2.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan2.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan2.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 2)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan3=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan3.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan3.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan3.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan3.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 3)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan4=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan4.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan4.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan4.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan4.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 4)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan5=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan5.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan5.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan5.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan5.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 5)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan6=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan6.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan6.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan6.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan6.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 6)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan7=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan7.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan7.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan7.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan7.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 7)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan8=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan8.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan8.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan8.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan8.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria+= np.count_nonzero(expert_num_tray == 8)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEplus-5zone-hot-discrete-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m obs,_\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generador\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan9=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan9.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan9.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan9.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan9.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = np.genfromtxt(\"observations_5zone.csv\",delimiter=\"\\t\",dtype=str)\n",
    "expert_actions = np.genfromtxt('actions_5zone.csv', dtype=np.int32)\n",
    "expert_next_observations = np.genfromtxt('next_observations_5zone.csv',delimiter=\"\\t\",dtype=str)\n",
    "expert_rewards = np.genfromtxt('rewards_5zone.csv', dtype=np.int32)\n",
    "expert_num_tray=np.genfromtxt('n_trayectoria_5zone.csv', dtype=np.int32)\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_observations = np.core.defchararray.replace(expert_observations, ',', ' ')\n",
    "\n",
    "# Reemplazar las comas\n",
    "expert_next_observations = np.core.defchararray.replace(expert_next_observations, ',', ' ')\n",
    "\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "# Crear un nuevo array para almacenar los datos convertidos\n",
    "converted_next_observations = np.genfromtxt(expert_observations, delimiter=' ', dtype=float)\n",
    "\n",
    "\n",
    "rewards=np.array(expert_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", converted_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)\n",
    "print(\"\\n\\t Estados siguientes: \\n\", converted_next_observations)\n",
    "print(\"\\n\\t Recompensas:\" ,expert_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_trayectoria += np.count_nonzero(expert_num_tray == 9)\n",
    "print(longitud_trayectoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_observations=converted_observations[0:longitud_trayectoria]\n",
    "expert_actions=expert_actions[0:longitud_trayectoria]\n",
    "converted_next_observations=converted_next_observations[0:longitud_trayectoria]\n",
    "expert_rewards=expert_rewards[0:longitud_trayectoria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el dataset [s,a,s',r] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "expert_rewards = expert_rewards.reshape(-1, 1)\n",
    "\n",
    "dataset1=np.concatenate([converted_observations,expert_a_one_hot],axis=1)\n",
    "dataset2=np.concatenate([dataset1,converted_next_observations],axis=1)\n",
    "dataset=np.concatenate([dataset2,expert_rewards],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = NormalizeObservation(env)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a, s', r] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, next_observations, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, converted_observations, expert_actions, converted_next_observations, expert_rewards, observations, actions, next_observations, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de RI-GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan10=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan10.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "BATCH_SIZE= round(len(X_train)/ITERATIONS)\n",
    "\n",
    "history=gan10.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de RI-GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan10.evaluate_D(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos al Generador \n",
    "rewardMean=gan10.evaluate_G()\n",
    "print('\\nRecompensa de Media:', rewardMean, '\\n') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
